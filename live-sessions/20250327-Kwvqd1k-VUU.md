# 2025-03-26 Week 11 - Session 2 - TDS Jan 25

[![2025-03-26 Week 11 - Session 2 - TDS Jan 25](https://i.ytimg.com/vi_webp/Kwvqd1k-VUU/sddefault.webp)](https://youtu.be/Kwvqd1k-VUU)

Duration: 1h 27m

Here's an FAQ based on the tutorial:

**Q1: I'm having trouble with some GA questions where even ChatGPT isn't giving the correct answers. Will ChatGPT be able to solve all answers correctly 100% after I complete the project?**

**A1:** The assignments and their solutions were created by Anand using ChatGPT. He's very skilled in prompt engineering. While ChatGPT can be powerful, achieving 100% accurate results consistently requires significant prompt engineering skill.

**Q2: I'm having trouble understanding this project as it seems more advanced than Project 1. Also, the audio was unclear earlier.**

**A2:** Yes, this project is more advanced. We also had some issues with the audio, but it's clear now. The deadline for GA7 is the 31st.

**Q3: I still can't see my Project 1 marks. Why is there a delay?**

**A3:** We're still working on grading Project 1. It's a massive task, and we underestimated the volume of data we needed to pull from Docker Hub. This led to several bottlenecks, requiring us to provision new VMs, local servers, and an NFS folder, and manage various credentials.

**Q4: For Project 2, some assignments involve deploying open-source LLMs. Do I need to replicate all of those?**

**A4:** Anand addressed this in a previous Wednesday session. You can find the entire discussion on the TDS playlist, and most of your questions about Project 2 should be answered there.

**Q5: Is there any update regarding my ROE (Result of Evaluation)? It's been a while.**

**A5:** Yes, I previously informed JK about your case and have just sent him another email reminder during this call. Please check your mail; he's been copied in.

**Q6: GA7 is due tonight. I've given up on it because I didn't understand the first question. What about others?**

**A6:** It's understandable if you found it challenging. For everyone else, please let us know if you've started GA7 or have decided not to.

**Q7: I've started GA7 but have more questions.**

**A7:** Please go ahead with your questions.

**Q8: Was there a session on Project 2 deployment?**

**A8:** Yes, Saranj conducted a session on Project 2 deployment on Tuesday. It hasn't been uploaded yet but will be available on the website within 15-30 minutes after this session concludes.

**Q9: Does Project 2 deployment, as discussed, involve using a free Azure account?**

**A9:** Jivraj has already done a deployment demo using DigitalOcean, not specifically Azure. However, you can achieve similar results with DigitalOcean. Jivraj also covered how to get an Azure education pack in a previous session, where he then demonstrated using DigitalOcean. You can find that session from last Tuesday.

**Q10: The Project 2 output requires a JSON string as the answer, but on Discord, it was suggested not to use escape characters like backslashes. What's the conclusion?**

**A10:** From a technical perspective, our script will send a request to your endpoint, which must respond with a JSON string. We then use `json.loads()` to parse this string. If `json.loads()` successfully parses it into a clean, correctly formatted JSON object, that's considered the correct answer. This applies even if you need to use escape characters like `\` within the string for proper formatting (e.g., newlines or quotes). You must ensure your response string is formatted in such a way that `json.loads()` can correctly parse it and maintain any necessary structure (like indentation in code).

**Q11: My GA's return statement is a raw JSON object (not a string with JSON inside), and the API response is also a raw JSON. Why am I getting an error?**

**A11:** The error indicates that the object must be a string. You must send a _string_ back to us, not a raw JSON object. The content of the "answer" field in your response must always be a string, as specified in the project requirements. This applies even for code (like Python programs), dictionaries, or lists—you must convert everything into a string because our system will then use `json.loads()` to parse it.

**Q12: How does `json.loads()` handle indentation when I send code in a string? Will it just convert newlines to `\n` without preserving actual indentation?**

**A12:** `json.loads()` will preserve indentation and proper formatting if the formatting (including newlines, tabs, spaces) is correctly encoded within the string you send. You _must_ include the necessary encoding for formatting in your string. We cannot guess the intended formatting. This ensures our system correctly interprets your code.

**Q13: I tried applying `json.loads()` to a JSON response from a GA question, but I'm getting an error that says "object must be a string".**

**A13:** The error confirms that your response from the endpoint needs to be a string, not a raw JSON object. When using `json.loads()`, you should extract and load only the _value_ associated with the "answer" key from your JSON response. For example, if your JSON is `{"answer": "{...}"}`, you would load the string representation of `{...}`.

**Q14: How will GA7 be graded?**

**A14:** GA7 will be graded on four components, each worth 25% of the total score:

1.  **Peer Evaluation (25%):** You will evaluate three peers within 10 days. Each review is worth 6 marks (total 18 marks). If you complete all three reviews, you get an additional 7 bonus marks, bringing this component's score to 25%. You must submit your own GA7 to perform peer reviews.
2.  **Good Reviews (25%):** Your evaluation of a peer is "good" if your score is within ±33% of the average score given by the other two reviewers. Each "good review" earns you 6 marks. If you provide three "good reviews," you get an additional 7 bonus marks, bringing this component's score to 25%.
3.  **Qualitative Assessment (25%):** This evaluates your GA submission's analysis (0-3 marks per question).
    - **0 marks:** Incorrect interpretation.
    - **1 mark:** Basic interpretation.
    - **2 marks:** Unique insight (not immediately obvious), supported by data.
    - **3 marks:** "Above and beyond" by incorporating external information sources.
    - **Bonus for your submission:** 3 bonus marks for 10-14 marks, or 7 bonus marks for 15-18 marks.
4.  **Quantitative Assessment (25%):** This checks the accuracy of your underlying data calculations. If your submission correctly computes the specific value for each of the 6 questions (3 marks each, total 18), you get 18 marks. If you achieve all 18 marks, you get a 7-mark bonus. This component applies to both your submission and your peer evaluations.

**Q15: For GA4, question 10, it mentions manual corrections for output from a PDF. How can we perform manual corrections?**

**A15:** Initially, the grading for this question was very strict. However, the criteria have been relaxed, allowing for more errors in your submission. Therefore, explicit "manual corrections" on your part are not required. We will be more lenient with errors for questions where manual correction was previously implied.

**Q16: For Markdown outputs, if the response contains hashes, spaces, or escape sequences (`\n`), what format should it be in? Sometimes it's line-by-line, other times a single paragraph.**

**A16:** For Markdown, you must send a string that is correctly formatted with the appropriate encoding (e.g., `\n` for newlines, `\t` for tabs). Our evaluation script will handle parsing these encoded formats correctly.

**Q17: For image files that are textual (PNG), will we be saving and sending the actual file?**

**A17:** Yes, you will send the actual image file. For PNGs, you might need to use a JavaScript library. We have tested sending physical files via a POST request to a Vercel endpoint, and it works. We recommend trying this first. If it works, you can then proceed with more complex processing for those files.

**Q18: Will there be support sessions for Project 2 deployment issues, similar to Project 1?**

**A18:** We've already held many sessions for Project 2, so it shouldn't be necessary. However, if you have specific demonstration requests for tomorrow's 8-10 PM session, please post them on Discord so we can address them.

**Q19: Is GA7 mandatory?**

**A19:** No, none of the GAs are strictly mandatory. You only need to complete four out of seven GAs and score 40% or more to be eligible for the end-term.

**Q20: I'm unsure which deployment platform (e.g., Azure, Vercel) is best for handling all GA questions, especially those involving subprocesses or file transfers. Many platform have limitations.**

**A20:** There's no single "best" platform; each has pros and cons. You've already tried Azure DevOps. Other options include GitHub Actions and GitLab Runners. If you encounter specific errors during deployment, please post them on Discord, and we can help troubleshoot them. Many platforms do have limitations regarding sub-processes, file sizes, and background endpoints.

**Q21: For GitHub-related questions, my functions work locally, but they're not working on Azure.**

**A21:** For GitHub-related questions, you don't have to use CLI commands or sub-processes. You can interact directly with the GitHub API. We demonstrated this in a previous session. This allows you to perform almost all GitHub actions by interacting with the API directly, which should avoid sub-process issues. We have tested sending files to Vercel endpoints, and it works if you try that first.

**Q22: For GA3, question 3, I need a JSON output. I processed my JSON using `json.loads()` and got the correct result during assessment. However, the endpoint response shows escape characters. What should I do?**

**A22:** The output shows escape characters because the JSON is being represented as a string. Our system will ensure it handles these formatting differences correctly during processing. As long as your JSON loads correctly with `json.loads()`, our system will pass it. You need to ensure the JSON body loads correctly.

**Q23: I don't know which platform is good for deployment to handle all these questions, especially considering limitations at various deployment portals.**

**A23:** There's no single "best" platform. Each framework has its pros and cons. If you've tried a few, share them on Discord, and we can offer recommendations based on your specific needs. Regarding deployment limitations at various portals (like Azure's sub-process or file size limits), we are aware of these. We are trying to find optimal solutions.

**Q24: How should I handle deployment for questions involving large files, like the 60MB file in one of the GAs?**

**A24:** The 60MB file in the GA is a static file that everyone receives and it won't change. You can pre-process this file beforehand on your local system, perhaps by creating a smaller, processed version (e.g., in CSV format). Then, when the request comes in, you can process the pre-processed smaller file to respond quickly. This way, you avoid processing a large file every time a request is made, effectively working around the deployment limitations.

**Q25: The Python code for GitHub questions works locally, but not on Azure. It seems to be related to CLI questions.**

**A25:** Yes, this issue often arises due to platform limitations with CLI-type questions and sub-processes. We understand it's not working on Azure. You can try to use the GitHub API directly instead of CLI commands. We've demonstrated how to interact with the GitHub API for such questions in a previous session. This would avoid relying on sub-processes, which are often restricted on deployment platforms.
