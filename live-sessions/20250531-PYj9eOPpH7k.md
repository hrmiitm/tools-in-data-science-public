# 2025 05 29 Week 3 Session 4 - TDS May 2025

[![2025 05 29 Week 3 Session 4 - TDS May 2025](https://i.ytimg.com/vi_webp/PYj9eOPpH7k/sddefault.webp)](https://youtu.be/PYj9eOPpH7k)

Duration: 9863.0Here's an FAQ summary of the tutorial:

---

### **OpenAI API & Function Calling: Frequently Asked Questions**

**Q1: What will this tutorial cover?**

**A1:** We'll dive into practical applications of Tools in Data Science (TDS), specifically focusing on OpenAI API calls. The core topics include making HTTPX and API calls, working with Base64 embeddings, understanding vector databases and RAG (Retrieval Augmented Generation), exploring multimodal embeddings using Ginja AI, and implementing function calling. We'll build a chatbot with conversation history and demonstrate extracting product details from images.

**Q2: What is the main problem word embeddings solve, and how do they work?**

**A2:** Word embeddings address the challenge of computers understanding the _meaning_ of words, rather than just their literal form. Traditional programming struggles with semantic meaning (e.g., recognizing "food" and "foot" are different despite similar spelling, or that "apple" and "fruit" are related). Word embeddings convert words into numerical vectors (like `[0.8, 0.9, 0.2]`), where each number represents a different feature or aspect of the word's meaning. This allows LLMs to understand relationships and make decisions based on these numerical representations, solving the semantic meaning problem.

**Q3: How do Large Language Models (LLMs) like ChatGPT handle conversation history and context?**

**A3:** LLMs fundamentally don't have built-in memory of past interactions within a conversation. To enable continuous, context-aware dialogue, you must explicitly send the entire conversation history (previous user queries and AI responses) along with each new user query to the LLM. This allows the model to "remember" the context you're providing for the current turn.

**Q4: How do I get started with the OpenAI API â€“ choosing a model and endpoint?**

**A4:** First, visit the OpenAI website and check the "Pricing" section to identify the most cost-effective models for your needs. Once you've selected a model (e.g., `gpt-4.0-nano`), go to the "Documentation" and navigate to the "Models" section. Here, you'll find details on various API endpoints like "Chat Completion" (older, widely used) and "Response" (newer, with features like audio and code interpretation). I'll demonstrate using "Chat Completion" for this tutorial.

**Q5: What's the process for making an API call to OpenAI from my code?**

**A5:** You can make API calls using `curl` commands or Python libraries like `requests` or `httpx`. The typical API request involves:

1.  **URL:** The specific endpoint you're targeting.
2.  **Headers:** Including an `Authorization` header with your API key (prefixed with "Bearer").
3.  **JSON Data:** A payload containing your request (e.g., the model name, messages for the chatbot).
    For security, it's best to store your API key as an environment variable (e.g., in `~/.bashrc` on Linux/macOS or system environment variables on Windows) rather than embedding it directly in your code. You'd then access it using `os.getenv("OPENAI_API_KEY")`.

**Q6: How do I structure conversation history and instructions for the LLM within an API call?**

**A6:** You pass history as a list of dictionaries, where each dictionary represents a turn in the conversation. Each dictionary has two keys: `role` and `content`.

- `role`: Can be "system" (for initial instructions like "answer in 3 words"), "user" (for user queries), or "assistant" (for AI responses).
- `content`: The actual message text.
  By appending each new user query and subsequent AI response to this list, you build the conversation context that's sent with every API call.

**Q7: How can I integrate multimodal capabilities, such as processing images, with LLMs?**

**A7:** For multimodal tasks, like extracting information from images, you'll typically use specialized APIs (e.g., Ginja API). The image itself is often converted into a Base64 encoded string, which is then sent within the API request's payload, alongside any accompanying text prompts or questions. This allows the LLM to "see" and interpret the image data.

**Q8: What is "function calling" and how can I implement it?**

**A8:** Function calling enables an LLM to interact with external tools or functions based on a user's prompt. You define a "function schema" (a structured JSON object describing the function's name, purpose, and required parameters). When the LLM receives a query that aligns with a defined function's purpose, it generates a structured call to that function (including parameter values). Your application then receives this function call, executes the corresponding code, and can feed the result back to the LLM if needed. This allows LLMs to perform actions beyond just generating text.
