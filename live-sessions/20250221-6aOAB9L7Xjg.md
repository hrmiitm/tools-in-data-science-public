# 2025-02-18 Week 6 - Session 3 - TDS Jan 25

[![2025-02-18 Week 6 - Session 3 - TDS Jan 25](https://i.ytimg.com/vi_webp/6aOAB9L7Xjg/sddefault.webp)](https://youtu.be/6aOAB9L7Xjg)

Duration: 1h 34m

Here's an FAQ based on the live tutorial:

**Q1: What is the most efficient tool for processing large JSON files, especially for tasks like checking if a key exists or how many times it appears?**

**A1:** You can use batch processing tools like `ijson`. It's the most efficient choice, particularly for large files, because it processes data in smaller batches rather than loading everything into memory at once.

**Q2: If the file isn't extremely large, is batch processing still necessary or just good practice?**

**A2:** While not strictly necessary for smaller files to avoid memory issues, it's good practice. In real-world scenarios, you'll often encounter large datasets, and knowing how to manage them efficiently using tools like `ijson` is a valuable skill.

**Q3: How does batch processing work to save memory?**

**A3:** Batch processing doesn't load the entire file into memory simultaneously. Instead, it retrieves a few items, processes them sequentially, and then loads the next set of items. This method significantly reduces memory consumption.

**Q4: Can I use standard SQL queries in DuckDB?**

**A4:** Yes, you can run proper SQL queries directly in DuckDB.

**Q5: What is the purpose of the `UNNEST` function in DuckDB when dealing with JSON arrays?**

**A5:** The `UNNEST` function transforms an array (or list) within your data into a tabular format. Each element of the array becomes a separate row in a new, temporary table, often alongside the `post_id` or other relevant identifiers. This allows you to apply standard SQL conditions and filters to individual elements of the array, which you can't do directly on a list.

**Q6: I'm trying to view the output of an `UNNEST` query in my console, but it's showing JSON instead of a table format. Why is that happening?**

**A6:** This can sometimes be a browser-specific rendering behavior. If you update your browser (e.g., Chrome) or try a different one (like Edge), it might display the output in a table-like format. Regardless of the display, internally, DuckDB treats the `UNNEST` output as a table, allowing you to apply further SQL operations.

**Q7: In Question 7, the task involves filtering posts with specific criteria on their comments. The initial approach only checks the 0th comment in the `comments` array. Why is this incorrect, and how should it be handled?**

**A7:** The problem statement implies that a post should be selected if _any_ of its comments meet the criteria (e.g., "at least one comment" with "two useful stars"). Your initial approach only checks the first (0th) comment in the array. To correctly implement this, you need to use `UNNEST` to convert the `comments` array into a table. Then, you can apply your filters (e.g., `stars.useful > 2` and `timestamp > specific_time`) on this unnested data, typically within a subquery, to ensure _any_ qualifying comment from a post is considered.

**Q8: I'm having trouble with Question 8 and getting an `array_length` mismatch error.**

**A8:** This error usually indicates an issue with how you're structuring your query, possibly related to `array_length` or how filters are applied to arrays. It's recommended to formulate your query step-by-step. First, run a basic `SELECT *` query to understand the exact structure of your data. Then, gradually add clauses and filters, checking the output in the console at each step. This iterative approach helps pinpoint where the mismatch or error is occurring and how to correctly apply filters.

**Q9: In Question 7, when saving an image as WebP using PIL, I got an error. How can I resolve this?**

**A9:** When saving WebP images, PIL often defaults to lossy compression. To avoid errors or unexpected behavior, you should explicitly set `lossless=True` in the `save()` method. For example: `image.save("output.webp", lossless=True)`.

**Q10: For Question 7, when comparing text-based output, should punctuation like dots and commas be included in the expected text?**

**A10:** Yes, for text-based comparisons, all punctuation marks, including dots and commas, are typically part of the expected text and should be included for an exact match.

**Q11: How do I get an API key for the Gemini API, and how do I use it to transcribe audio?**

**A11:** You can generate a Gemini API key from the Google AI Studio. Once you have the key, you can use a Python library (like the `google.generativeai` library) to interact with the API. You would convert your audio file (e.g., from YouTube using `yt-dlp`) to a format like Base64, then send it to the Gemini API with your key for transcription.

**Q12: After transcribing audio using Gemini, the text output might not be perfect. Can I correct it, and does the evaluation script tolerate minor mistakes?**

**A12:** Yes, the Gemini API output might not be 100% accurate, and you can manually correct the text after transcription. The evaluation script for such tasks typically tolerates _one_ mistake in the transcribed text. If there are more errors, it won't be marked as correct. So, it's often necessary to review the transcribed text and make minor corrections to align with the expected answer.
